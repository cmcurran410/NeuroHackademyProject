{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Retrieval from Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Aspects\n",
    "#### How does a computer understand semantic information from written language?\n",
    "\n",
    "I want to essentially create a short/basic non-scientific text sample to initially make sure that the computer is properly breaking down and processing each element of the text the way that I need it to\n",
    "\n",
    "**SAMPLE TEXT DATA:**\n",
    "create a short paragraph (6-8 sentences):\n",
    "\n",
    "- all the sentences belong in the same larger story; however, they do not all share the same subtopic (AKA the keyword)\n",
    "- have a key word in mind\n",
    "- sentence which directly contain the key word---> 2 sentences \n",
    "    - The rabbit looked at the goose.\n",
    "    - It lived near the rabbit.\n",
    "- sentence which is completely unrelated and does NOT contain the key word---> 2 sentences\n",
    "    - There was a goose in the garden\n",
    "    - A snake was slithering in the sun slowly.\n",
    "- sentence which is somewhat semantically related but does NOT contain the key word---> 2 sentences\n",
    "    - It then hopped over the fence at 1:15 PM.\n",
    "    - The former was 1/5 the height of the latter.\n",
    "- sentence which incorporates a synonym of the key word but not explicitly the keyword itself\n",
    "    - A hare was relaxing in the shade.\n",
    "- sentence containing numerical characters\n",
    "    - time: \"at 1:15 PM\"\n",
    "    - \"The year was 2004.\"\n",
    "    - statistical in nature: \"1/5 the height\"\n",
    "\n",
    "\n",
    "\n",
    "EX: \"There was a goose in the garden. The rabbit looked at the goose. It then hopped over the fence at 1:15 PM. A hare was relaxing in the shade. It lived near the rabbit. The former was 1/5 the height of the latter. A snake was slithering in the sun slowly. The year was 2004.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter sample text data as input\n",
    "text = \"There was a goose in the garden. The rabbit looked at the goose. It then hopped over the fence at 1:15 PM. A hare was relaxing in the shade. It lived near the rabbit. The former was 1/5 the height of the latter. A snake was slithering in the sun slowly. The year was 2004.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing using SpaceCy\n",
    "\n",
    "Industrial Strength Cython Tool for NLP\n",
    "\n",
    "the main question is which English pipeline do we want to use? There is a trade between how complex the pipeline is and the speed that it takes to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the large model\n",
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "doc_lg = nlp_lg(\"Sally speaks English\")\n",
    "\n",
    "\n",
    "# Print named entities \n",
    "print(\"Large model named entities:\", [(ent.text, ent.label_) for ent in doc_lg.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization vs Embedding\n",
    "\n",
    "as you can see, the output for embedding is printing a vector representation for each token instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The cat sat on the mat.\")\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "\n",
    "# EMBEDDING\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The cat sat on the mat.\")\n",
    "embeddings = [token.vector for token in doc]\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing BERT\n",
    "### Implementing a Transformer rather than Tokenizer\n",
    "\n",
    "After seeing the flaws in Named Entity Recognition from the Tokenizers in SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "# Adam optimizer with weight decay regularization\n",
    "# prevents overfitting and helps the model generalize better\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# PyTorch utility to handle and batch data better\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a number of labels\n",
    "NUM_LABELS = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the BERT tokenizer and model with whole word masking\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking\")\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-large-cased-whole-word-masking\", num_labels=NUM_LABELS)  # Set NUM_LABELS according to your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Creation for NER\n",
    "\n",
    "(with B and I Tags- beginning and end of the category)\n",
    "\n",
    "\n",
    "### GENERAL/BASICS\n",
    "\n",
    "\n",
    "**People:**\n",
    "- B-PER\n",
    "- I-PER\n",
    "\n",
    "\n",
    "**Locations (Country):**\n",
    "- B-LOC\n",
    "- I-LOC\n",
    "\n",
    "\n",
    "**Geographic/Geological Formations:**\n",
    "- B-GEO\n",
    "- I-GEO\n",
    "\n",
    "\n",
    "**Animals:**\n",
    "- B-ANIM\n",
    "- I-ANIM\n",
    "\n",
    "\n",
    "**Numbers:**\n",
    "- B-NUM\n",
    "- I-NUM\n",
    "\n",
    "\n",
    "**Colors:**\n",
    "- B-COLOR\n",
    "- I-COLOR\n",
    "\n",
    "\n",
    "**University/Institution/Company:**\n",
    "- B-INSTIT\n",
    "- I-INSTIT\n",
    "\n",
    "\n",
    "**Scientific Terminology/Technique:**\n",
    "- B-SCI_TERM\n",
    "- I-SCI_TERM\n",
    "\n",
    "\n",
    "\n",
    "**General Subject/Topic/Field**\n",
    "- B-FIELD\n",
    "- I-FIELD\n",
    "\n",
    "\n",
    "**Statistical Tests or Analysis:**\n",
    "- B-STAT\n",
    "- I-STAT\n",
    "\n",
    "\n",
    "**Mathematical Concepts:**\n",
    "- B-MATH\n",
    "- I-MATH\n",
    "\n",
    "\n",
    "**Programming Concepts:**\n",
    "- B-CODE\n",
    "- I-CODE\n",
    "\n",
    "\n",
    "**Percentages/Fractions:**\n",
    "- B-PERCENT\n",
    "- I-PERCENT\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "#### NEURO RESEARCH SPECIFIC\n",
    "\n",
    "\n",
    "**Neuroimaging Modalities:**\n",
    "- B-NEUROIMG\n",
    "- I-NEUROIMG\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "#### BASIC SCIENCE FIELDS/CONCEPTS\n",
    "\n",
    "\n",
    "**Chemical Compound, Ion Names, Neurotransmitters:**\n",
    "- B-CHEM\n",
    "- I-CHEM\n",
    "\n",
    "\n",
    "**Drugs:**\n",
    "- B-DRUG\n",
    "- I-DRUG\n",
    "\n",
    "\n",
    "\n",
    "**Diseases/Disorders:**\n",
    "- B-DISORD\n",
    "- I-DISORD\n",
    "\n",
    "\n",
    "**Biology (or Neuro) Concepts/Pathology/Anatomy:**\n",
    "- B-BIO\n",
    "- I-BIO\n",
    "\n",
    "\n",
    "**Physics Concepts:**\n",
    "- B-PHYSICS\n",
    "- I-PHYSICS\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "#### LANGUAGE/LINGUISTICS\n",
    "\n",
    "\n",
    "**Linguistic Concepts and Theories:**\n",
    "- B-LING\n",
    "- I-LING\n",
    "\n",
    "\n",
    "**Languages:**\n",
    "- B-LANG\n",
    "- I-LANG\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "#### Additional Labels:\n",
    "\n",
    "\n",
    "**Citations:**\n",
    "- B-CITE\n",
    "- I-CITE\n",
    "\n",
    "\n",
    "**Padding:**\n",
    "- PAD\n",
    "\n",
    "\n",
    "**Outside:**\n",
    "- O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all labels\n",
    "labels = [\n",
    "    \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-GEO\", \"I-GEO\",\n",
    "    \"B-ANIM\", \"I-ANIM\", \"B-NUM\", \"I-NUM\", \"B-COLOR\", \"I-COLOR\",\n",
    "    \"B-INSTIT\", \"I-INSTIT\", \"B-SCI_TERM\", \"I-SCI_TERM\",\n",
    "    \"B-FIELD\", \"I-FIELD\", \"B-STAT\", \"I-STAT\", \"B-MATH\", \"I-MATH\",\n",
    "    \"B-CODE\", \"I-CODE\", \"B-PERCENT\", \"I-PERCENT\", \"B-NEUROIMG\", \"I-NEUROIMG\",\n",
    "    \"B-CHEM\", \"I-CHEM\", \"B-DRUG\", \"I-DRUG\", \"B-DISORD\", \"I-DISORD\",\n",
    "    \"B-BIO\", \"I-BIO\", \"B-PHYSICS\", \"I-PHYSICS\", \"B-LING\", \"I-LING\",\n",
    "    \"B-LANG\", \"I-LANG\", \"B-CITE\", \"I-CITE\", \"PAD\", \"O\"\n",
    "]\n",
    "\n",
    "# Mapping labels to integers\n",
    "# we do this by creating a dictionary {}\n",
    "# start with the empty dict, label_map\n",
    "    # looping through the list 'labels'\n",
    "    # key:value so that label is the key and idx is the value\n",
    "    # iterate over pairs of idx, label\n",
    "label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "# Reverse mapping (optional, useful for decoding predictions)\n",
    "idx_to_label = {idx: label for label, idx in label_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "So that the algorithm can detect the Part Of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DT'), ('patient', 'NN'), ('has', 'VBZ'), ('Alzheimer', 'NNP'), (\"'s\", 'POS'), ('disease', 'NN'), ('.', '.')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/KeerthiStanley/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/KeerthiStanley/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Using the Natural Language ToolKit for this!\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Creating a function, pos_tagging()\n",
    "# input is text data\n",
    "def pos_tagging(texts):\n",
    "    # Create a list for storing the text post- POS tagging\n",
    "    tagged_texts = []\n",
    "    # for a given iteration in the text input\n",
    "    for text in texts:\n",
    "        # tokenize the string into each word\n",
    "        tokens = word_tokenize(\" \".join(text))\n",
    "        # apply POS tagging to each token\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        # add each tag to the initial list created\n",
    "        tagged_texts.append(pos_tags)\n",
    "    return tagged_texts\n",
    "\n",
    "# Example usage\n",
    "texts = [[\"The patient has Alzheimer's disease.\"]]\n",
    "pos_tagged_texts = pos_tagging(texts)\n",
    "print(pos_tagged_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Function to Load in Text Data, Have it Tokenized and the Label Each Corresponding Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a Small Sample Sentence/Phrase Input and Its Labels\n",
    "\n",
    "\" John Doe has Alzheimer's disease, which affects his brain. They used an MRI scan to diagnose this. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['John', 'Doe', 'has', 'Alzheimer', \"'s\", 'disease', ',', 'which', 'affects', 'his', 'brain', '.', 'They', 'used', 'an', 'MRI', 'scan', 'to', 'diagnose', 'this', '.']\n",
      "Labels: ['B-PER', 'I-PER', 'O', 'B-DISORD', 'I-DISORD', 'I-DISORD', 'O', 'O', 'O', 'O', 'B-BIO', 'O', 'O', 'O', 'O', 'B-NEUROIMG', 'I-NEUROIMG', 'O', 'O', 'O', 'O']\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize sentences into words using NLTK\n",
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "# Example sentences and their labels\n",
    "sentences = [\n",
    "    \"John Doe has Alzheimer's disease, which affects his brain. They used an MRI scan to diagnose this.\"\n",
    "]\n",
    "\n",
    "tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "# Example labels corresponding to tokens (manually for demo purposes)\n",
    "labels = [\n",
    "    [\"B-PER\", \"I-PER\", \"O\", \"B-DISORD\", \"I-DISORD\", \"I-DISORD\", \"O\", \"O\", \"O\", \"O\", \"B-BIO\", \"O\",\n",
    "    \"O\", \"O\", \"O\", \"B-NEUROIMG\", \"I-NEUROIMG\", \"O\", \"O\", \"O\", \"O\"]\n",
    "]\n",
    "\n",
    "\n",
    "# # Print the tokens and labels \n",
    "# for token, label in zip(tokens, label_sequence):\n",
    "#     print(f\"Token: {token}, Label: {label}\")\n",
    "\n",
    "# Print the tokens and labels \n",
    "for sentence_tokens, sentence_labels in zip(tokenized_sentences, labels):\n",
    "    print(f\"Tokens: {sentence_tokens}\")\n",
    "    print(f\"Labels: {sentence_labels}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and preserve labels\n",
    "\n",
    "# INPUTS:\n",
    "    # sentences: a list of sentences where each sentence is a list of words\n",
    "    # corresponding labels for each token of the sequence\n",
    "def tokenize_and_preserve_labels(sentences, text_labels):\n",
    "    # creates two empty storage lists for tokenized sentences and for the labels\n",
    "    tokenized_sentences = []\n",
    "    label_list = []\n",
    "    \n",
    "    # for each iteration of the pairs of sentences and their corresponding labels...\n",
    "    for sentence, labels in zip(sentences, text_labels):\n",
    "        # store the tokenized sentence and it's label\n",
    "        tokenized_sentence = []\n",
    "        sentence_labels = []\n",
    "        # for each iteration of the pairs of sentences and their corresponding labels...\n",
    "        for word, label in zip(sentence, labels):\n",
    "            # using BERT's tokenizer to break down the word into subwords\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            # then we measure the length of the tokenized word list to get the # of subwords\n",
    "            n_subwords = len(tokenized_word)\n",
    "            # add each tokenized word list to the tokenized sentence list\n",
    "            tokenized_sentence.extend(tokenized_word)\n",
    "            # [label] = the label for THAT iteration of the loop\n",
    "            # multiply it by the number of subwords so that even if the word is split into smaller parts, it maintains the same label throughout\n",
    "            sentence_labels.extend([label] * n_subwords)\n",
    "        tokenized_sentences.append(tokenized_sentence) # append the tokenized sentence from that iteration onto the longer list\n",
    "        label_list.append(sentence_labels) # append the label from that iteration onto the longer list\n",
    "    return tokenized_sentences, label_list\n",
    "\n",
    "# Call the function with example data\n",
    "tokenized_sentences, preserved_labels = tokenize_and_preserve_labels(tokenized_sentences, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NERDataset Class: Format to Make Training Compatible with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum length of each input sentence accepted by the model\n",
    "# will be padded if shorter than this length\n",
    "MAX_LEN = 128\n",
    "# the number of samples processed simultaneously in a forward/backward pass during training or inference\n",
    "# larger batch sizes = faster training but also require more memory\n",
    "BATCH_SIZE = 60\n",
    "# number of complete passes through the training data\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now that my parameters are established, I'm going to establish the NER Dataset class\n",
    "\n",
    "#### the Dataset class imported, is a base class that you can inherit from to create your own custom datasets\n",
    "\n",
    "There are other common datasets which can be used to training our model, but this allows us to train the model with our own design\n",
    "\n",
    "Some examples of specfic pre-made datasets:\n",
    "- SemEval Datasets: Semantic Analysis\n",
    "- TREC (Text REtrieval Conference): Question Classification\n",
    "- SQuAD (Stanford Question Answering Dataset)\n",
    "- Common Crawl: Used for Chat GPT\n",
    "\n",
    "Bio specific:\n",
    "- PubMed Dataset\n",
    "- BioNLP / BioBERT\n",
    "- NeuroSynth\n",
    "- ScispaCy: these models are trained on scientific texts and include pretrained NER models that can identify entities like chemicals, genes, and organisms. The datasets used to train these models (like CRAFT or JNLPBA) are labeled *\n",
    "\n",
    "NCBI Disease Corpus:\n",
    "\n",
    "dataset specifically for disease named entity recognition, containing clinical and biomedical texts with annotations for disease names.\n",
    "\n",
    "MedMentions:\n",
    "\n",
    "dataset that includes a large number of medical mentions, with annotations for entities such as diseases, drugs, and genes.\n",
    "\n",
    "PubTator Central:\n",
    "provides annotations for a large number of biomedical articles, including entities like genes, diseases, and chemicals.\n",
    "Use: Can be used to train models for biomedical text mining and entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Initialize the NERDataset class.\n",
    "\n",
    "        Parameters:\n",
    "        - sentences (list of list of str): Tokenized sentences.\n",
    "        - labels (list of list of str): Corresponding labels for each token in the sentences.\n",
    "        - tokenizer (transformers.PreTrainedTokenizer): Tokenizer used to convert text into token IDs.\n",
    "        - max_len (int): Maximum sequence length for padding/truncation.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences  # Store the list of tokenized sentences.\n",
    "        self.labels = labels  # Store the list of corresponding labels.\n",
    "        self.tokenizer = tokenizer  # Store the tokenizer for later use.\n",
    "        self.max_len = max_len  # Store the maximum length for padding/truncation.\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of sentences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - int: Number of sentences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch the item at the given index, tokenize it, and align the labels.\n",
    "\n",
    "        Parameters:\n",
    "        - idx (int): Index of the item to fetch.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary containing 'input_ids', 'attention_mask', and 'labels' tensors.\n",
    "        \"\"\"\n",
    "        sentence = self.sentences[idx]  # Get the sentence at the specified index.\n",
    "        label = self.labels[idx]  # Get the corresponding labels for the sentence.\n",
    "        \n",
    "        # Tokenize the sentence and align labels\n",
    "        tokens = self.tokenizer(\n",
    "            sentence, \n",
    "            max_length=self.max_len, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            is_split_into_words=True\n",
    "        )\n",
    "        \n",
    "        input_ids = tokens['input_ids']  # Extract input token IDs from tokenized output.\n",
    "        attention_mask = tokens['attention_mask']  # Extract attention mask from tokenized output.\n",
    "        \n",
    "        # Initialize labels with -100 for padding (ignored in loss calculation).\n",
    "        labels = [-100] * self.max_len\n",
    "        # Align labels with the tokenized words.\n",
    "        for i, label_id in enumerate(label):\n",
    "            if i < self.max_len:\n",
    "                labels[i] = label_map.get(label_id, -100)  # Map label to its integer representation.\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),  # Convert input IDs to a PyTorch tensor.\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),  # Convert attention mask to a PyTorch tensor.\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)  # Convert labels to a PyTorch tensor.\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "# Create your dataset\n",
    "dataset = NERDataset(tokenized_sentences, preserved_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "# DataLoader\n",
    "# Initialize DataLoader to handle batching and shuffling of the dataset.\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set up a learning rate scheduler\n",
    "total_steps = len(data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.6293100118637085\n",
      "Epoch 2/10, Loss: 2.0066909790039062\n",
      "Epoch 3/10, Loss: 1.3336238861083984\n",
      "Epoch 4/10, Loss: 0.7771637439727783\n",
      "Epoch 5/10, Loss: 0.5990918874740601\n",
      "Epoch 6/10, Loss: 0.54462069272995\n",
      "Epoch 7/10, Loss: 0.4330599904060364\n",
      "Epoch 8/10, Loss: 0.314517080783844\n",
      "Epoch 9/10, Loss: 0.2986332178115845\n",
      "Epoch 10/10, Loss: 0.22897063195705414\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## TRAINING LOOP\n",
    "## generate the loss per batch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.13250534236431122\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "# let's use an Evaluation Loop\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE PREDICTIONS\n",
    "\n",
    "def predict(sentence):\n",
    "    tokens = tokenizer(sentence, return_tensors='pt', truncation=True, padding='max_length', max_length=MAX_LEN)\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Decode predictions\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()[0]  # Get the first (and only) sequence\n",
    "    predicted_labels = [idx_to_label[id] for id in predictions if id != -100]  # Ignore padding tokens\n",
    "    \n",
    "    return predicted_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVE RELEVANT SENTENCES\n",
    "\n",
    "def retrieve_relevant_sentences(text, keyword):\n",
    "    # Split text into sentences based on periods\n",
    "    sentences = text.split('.') \n",
    "    relevant_sentences = []\n",
    "    \n",
    "    # Map keyword to its label if it exists in your label set\n",
    "    keyword_label = None\n",
    "    for label in labels:\n",
    "        if keyword in label:\n",
    "            keyword_label = label\n",
    "            break\n",
    "    \n",
    "    # Ensure that the keyword_label is valid\n",
    "    if not keyword_label:\n",
    "        print(\"Keyword not found in label set.\")\n",
    "        return relevant_sentences\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # Predict the labels for the sentence\n",
    "        predictions = predict(sentence)\n",
    "        \n",
    "        # Map prediction indices back to labels\n",
    "        predicted_labels = [idx_to_label[idx] for idx in predictions[0]]\n",
    "        \n",
    "        # Check if the keyword_label is in the predicted labels\n",
    "        if keyword_label in predicted_labels:\n",
    "            relevant_sentences.append(sentence)\n",
    "    \n",
    "    return relevant_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor.__contains__ only supports Tensor or scalar, but you passed in a <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mJohn Doe has Alzheimer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms disease, which affects his brain. They used an MRI scan to diagnose this.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mThe treatment involves medication and therapy.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Example keyword for testing\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m relevant_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_relevant_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevant Sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m relevant_sentences:\n",
      "Cell \u001b[0;32mIn[132], line 11\u001b[0m, in \u001b[0;36mretrieve_relevant_sentences\u001b[0;34m(text, keyword)\u001b[0m\n\u001b[1;32m      9\u001b[0m keyword_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mkeyword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m:\n\u001b[1;32m     12\u001b[0m         keyword_label \u001b[38;5;241m=\u001b[39m label\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:1093\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1088\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[1;32m   1089\u001b[0m ):\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (element \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1095\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor.__contains__ only supports Tensor or scalar, but you passed in a <class 'str'>."
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "John Doe has Alzheimer's disease, which affects his brain. They used an MRI scan to diagnose this.\n",
    "The treatment involves medication and therapy.\n",
    "\"\"\"\n",
    "keyword = \"John\"  # Example keyword for testing\n",
    "\n",
    "relevant_sentences = retrieve_relevant_sentences(text, keyword)\n",
    "\n",
    "print(\"Relevant Sentences:\")\n",
    "for sentence in relevant_sentences:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
